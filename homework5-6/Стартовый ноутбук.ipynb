{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.master" : "local[1]",
        "spark.stopList" : "ppkm kmt covid"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# Тестовый ноутбук<br>\n",
        "\n",
        "<div><span style=\"font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;;\"><br></span></div><div><span style=\"font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;;\">Попробуйте исполнить ячейки и убедитесь, что spark поднимается и работает.</span></div><div><span style=\"font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;;\">Если вы будете создавать свой ноутбук для ДЗ, скопируйте конфигурацию из этого ноутбука</span></div>\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703538359483,
          "endTs" : 1703538359987
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val sc = spark.sparkContext"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703538360855,
          "endTs" : 1703538361256
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// Функция для расчета числа Pi методом Монте-Карло\n",
        "\n",
        "val count = sc.parallelize(1 to 100000).filter { _ =>\n",
        "  val x = math.random\n",
        "  val y = math.random\n",
        "  x*x + y*y < 1\n",
        "}.count()\n",
        "println(s\"Pi is roughly ${4.0 * count / 100000}\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "Pi is roughly 3.14288\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703538362359,
          "endTs" : 1703538362862
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val df = spark.read.option(\"header\", \"true\").csv(\"housing.csv\")\n",
        "df.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n",
            "|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n",
            "|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n",
            "|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n",
            "|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n",
            "|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n",
            "|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703538364076,
          "endTs" : 1703538364465
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val df = spark.read.option(\"header\", \"true\").csv(\"datasets/ppkmSentiment/ppkm_dataset.csv\")\n",
        "df.show()\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+--------------------+\n",
            "|  class|             comment|\n",
            "+-------+--------------------+\n",
            "|positif|Kami siap laksana...|\n",
            "|positif|Siap melaksanakan...|\n",
            "|positif|Siap dukung dan s...|\n",
            "|positif|Langkah 3M ini su...|\n",
            "|positif|Siap amankan selu...|\n",
            "|positif|Siap utk di sosia...|\n",
            "|positif|Mendukung kebijak...|\n",
            "|positif|Mari bersama cega...|\n",
            "|positif|Mari kita suksesk...|\n",
            "|positif|Siap kawal dan am...|\n",
            "|positif|Semoga pak gubern...|\n",
            "|positif|Semoga bapak sela...|\n",
            "|positif|     Semangat selalu|\n",
            "|positif|Semoga berhasil.....|\n",
            "|positif|patuhi prokes dan...|\n",
            "|positif|terimakasih atas ...|\n",
            "|positif|Sehat selalu pemi...|\n",
            "|positif|Semoga selalu dib...|\n",
            "|positif|semoga pandemi se...|\n",
            "|positif|Makasih bu, sukse...|\n",
            "+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703544938853,
          "endTs" : 1703544942547
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "/*\n",
        "Плюсы MSJ:\n",
        "    1. Если данные не умещаются в памяти, MSJ может обрабатывать большие объемы данных, используя настройки persist и caching.\n",
        "    2. MSJ может быть использован для присоединения двух или более таблиц, в то время как Hash Join - для двух таблиц.\n",
        "    3. В случае присоединения неупорядоченных данных, MSJ более эффективным, так как не требует преобразования данных в пары ключ-значение.\n",
        "    4. Позволяет сортировать данные во время операции соединения\n",
        "\n",
        "Недостатки MSJ:\n",
        "    1. Времени для выболнения требуется больше, чем у Hash Join, так как требуется дополнительное сортировка данных перед присоединением.\n",
        "    2. Время выполнения MSJ зависит от количества элементов в соединяемых списках, а также от порядка элементов в списках.\n",
        "*/\n",
        "\n",
        "val wcDf: DataFrame = df\n",
        "    // разделяем комментарий по несловообразующим символам и _, который нас тоже не устраивает \n",
        "    .select(explode(split($\"comment\", raw\"[\\W_]+\")).as(\"word\"))\n",
        "    .where($\"word\" =!= \"\")\n",
        "    // приведение к нижнему регистру\n",
        "    .withColumn(\"word\", lower($\"word\"))\n",
        "    .groupBy(\"word\")\n",
        "    .count()\n",
        "    .orderBy($\"count\".desc)\n",
        "\n",
        "val stopList: List[String] = spark.conf\n",
        "    .get(\"spark.stopList\")\n",
        "    .split(\" \")\n",
        "    .toList\n",
        "\n",
        "val updatedWordCountDF: DataFrame = wcDf\n",
        "    //Изменение порядка колонок\n",
        "    .select(wcDf(\"count\"), wcDf(\"word\"))\n",
        "    // исключение слов из конфига спарка\n",
        "    .filter(!($\"word\".isin(stopList: _*)))\n",
        "\n",
        "updatedWordCountDF.show(false)\n",
        "\n",
        "//Сохранение в формате tsv\n",
        "updatedWordCountDF.write\n",
        "    .mode(\"overwrite\")\n",
        "    .option(\"sep\", \"\\t\")\n",
        "    .csv(\"output5.tsv\")\n",
        "\n",
        "//чтение формата tsv\n",
        " val readDF: DataFrame = spark.read.option(\"sep\", \"\\t\").csv(\"output3.tsv\")\n",
        "readDF.show(false)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----+------------+\n",
            "|count|word        |\n",
            "+-----+------------+\n",
            "|111  |mikro       |\n",
            "|88   |dan         |\n",
            "|84   |co          |\n",
            "|83   |t           |\n",
            "|82   |https       |\n",
            "|78   |di          |\n",
            "|72   |ppkmmikro   |\n",
            "|70   |19          |\n",
            "|66   |masyarakat  |\n",
            "|60   |yg          |\n",
            "|44   |kegiatan    |\n",
            "|43   |pembatasan  |\n",
            "|43   |perpanjangan|\n",
            "|34   |rt          |\n",
            "|32   |ada         |\n",
            "|31   |berbasis    |\n",
            "|30   |yang        |\n",
            "|28   |maret       |\n",
            "|27   |2021        |\n",
            "|27   |untuk       |\n",
            "+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---+--------------+\n",
            "|_c0|_c1           |\n",
            "+---+--------------+\n",
            "|1  |tumbuh        |\n",
            "|1  |artinya       |\n",
            "|1  |tagihan       |\n",
            "|1  |erwin3977     |\n",
            "|1  |absurd        |\n",
            "|1  |bangun        |\n",
            "|1  |pembuat       |\n",
            "|1  |kabar         |\n",
            "|1  |gini          |\n",
            "|1  |tilang        |\n",
            "|1  |australia     |\n",
            "|1  |dilockdown    |\n",
            "|1  |kayanya       |\n",
            "|1  |kesabaran     |\n",
            "|1  |disekat       |\n",
            "|1  |kerjaannya    |\n",
            "|1  |serma         |\n",
            "|1  |grob          |\n",
            "|1  |lele          |\n",
            "|1  |ppkmmikrohttps|\n",
            "+---+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "Какие плюсы и недостатки у Merge Sort Join в отличии от Hash Join? (1 балл)\n",
        "Соберите WordCount приложение, запустите на датасете ppkm_sentiment (1 балл)\n",
        "Измените WordCount так, чтобы он удалял знаки препинания и приводил все слова к единому регистру (1 балл)\n",
        "Измените выход WordCount так, чтобы сортировка была по количеству повторений, а список слов был во втором столбце, а не в первом (1 балл)\n",
        "Измените выход WordCount, чтобы формат соответствовал TSV (1 балл)\n",
        "Добавьте в WordCount возможность через конфигурацию задать список стоп-слов, которые будут отфильтрованы во время работы приложения (2 балла)\n",
        "Выполните broadcast join на двух датасетах, не используя метод join(). Можно использовать любые предварительные трансформации. В качестве исходных данных возьмите Company.csv и Company_Tweet.csv из датасета Tweets about the Top Companies from 2005 to 2020 (3 балла)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703545192547,
          "endTs" : 1703545193060
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// HW 6\n",
        "def getData(filepath: String): DataFrame = {\n",
        "  spark.read\n",
        "    .option(\"header\", \"true\")\n",
        "    .csv(filepath)\n",
        "    .withColumn(\"label\", $\"label\".cast(\"int\"))\n",
        "    .filter($\"label\" === 1 || $\"label\" === 0)\n",
        "}\n",
        "\n",
        "val trainDataset: DataFrame = getData(\"datasets/IMDBSentimentAnalysis/Train.csv\")\n",
        "val testDataset: DataFrame = getData(\"datasets/IMDBSentimentAnalysis/Test.csv\")\n",
        "val validDataset: DataFrame = getData(\"datasets/IMDBSentimentAnalysis/Valid.csv\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703545337904,
          "endTs" : 1703545348571
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.ml.classification.LogisticRegression\n",
        "import org.apache.spark.ml.{Pipeline}\n",
        "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
        "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
        "\n",
        "val tokenizer = new Tokenizer()\n",
        "    .setInputCol(\"text\")\n",
        "    .setOutputCol(\"words\")\n",
        "val hashingTF = new HashingTF()\n",
        "    .setInputCol(tokenizer.getOutputCol)\n",
        "    .setOutputCol(\"features\").setNumFeatures(12000)\n",
        "val lr = new LogisticRegression()\n",
        "  .setMaxIter(10)\n",
        "  .setRegParam(0.3)\n",
        "val pipeline = new Pipeline()\n",
        "    .setStages(Array(tokenizer, hashingTF, lr))\n",
        "\n",
        "val model = pipeline.fit(trainDataset)\n",
        "val predictions = model.transform(validDataset)\n",
        "\n",
        "val evaluator = new MulticlassClassificationEvaluator()\n",
        "  .setMetricName(\"accuracy\")\n",
        "  \n",
        "val accuracy = evaluator.evaluate(predictions)\n",
        "println(s\"Accuracy = $accuracy\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "Accuracy = 0.8643835616438356\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    }
  ]
}